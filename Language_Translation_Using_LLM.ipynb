{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installations"
      ],
      "metadata": {
        "id": "SA_Fb80ZIaPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install langchain-huggingface\n",
        "!pip install langchain_community\n",
        "!pip install googletrans\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install deep-translator\n",
        "!pip install langchain-groq"
      ],
      "metadata": {
        "id": "bfHbTnjVtzxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "TrPXtdi2Ipe4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqZ86jXhttIL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "import datasets\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "# Setting device to GPU if GPU is available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "train_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\n",
        "valid_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"validation\")\n",
        "test_dataset  = load_dataset(\"cfilt/iitb-english-hindi\", split=\"test\")\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset_loaded, number_of_examples, prompt):\n",
        "  source_sentences = [entry[\"en\"] for entry in dataset_loaded['translation']][:number_of_examples]\n",
        "  target_sentences = [entry[\"hi\"] for entry in dataset_loaded['translation']][:number_of_examples]\n",
        "\n",
        "  prompted_sentences = [prompt.format(text) for text in source_sentences]\n",
        "\n",
        "  return prompted_sentences, source_sentences, target_sentences\n",
        "\n",
        "\n",
        "def evaluation_score(metric, predictions, references):\n",
        "  metric_loaded = evaluate.load(metric)\n",
        "  results = metric_loaded.compute(predictions=predictions, references=references)\n",
        "  return results[metric]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Initialize the Groq model\n",
        "groq_model = ChatGroq(model_name=\"llama3-8b-8192\", groq_api_key=GROQ_API_KEY)\n",
        "\n",
        "def translate_google(inputs, single_sentence=False):\n",
        "  output_google = []\n",
        "  if single_sentence == False:\n",
        "    for text in inputs:\n",
        "      translations = GoogleTranslator(source=\"en\", target=\"hi\").translate(text)\n",
        "      output_google.append(translations)\n",
        "  else:\n",
        "    translations = GoogleTranslator(source=\"en\", target=\"hi\").translate(inputs)\n",
        "    output_google.append(translations)\n",
        "\n",
        "  return output_google\n",
        "\n",
        "\n",
        "def translate_llama(inputs, single_sentence=False):\n",
        "  output_llama = []\n",
        "\n",
        "  prompt_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Variable to be replaced dynamically\n",
        "    template=\"Translate {text} to Hindi only\"\n",
        "  )\n",
        "\n",
        "  chain = LLMChain(llm=groq_model, prompt=prompt_template)\n",
        "\n",
        "  if single_sentence == False:\n",
        "    for input in inputs:\n",
        "      response = chain.run(text=input)\n",
        "      output_llama.append(response)\n",
        "  else:\n",
        "    response = chain.run(text=inputs)\n",
        "    output_llama.append(response)\n",
        "\n",
        "  return output_llama\n"
      ],
      "metadata": {
        "id": "D4jGY5S1tuIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model Translations - Google Translate, Llama"
      ],
      "metadata": {
        "id": "eZtO2TkwIw44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Translate to Hindi: {}. Translation:\"\n",
        "prompt_inputs, source_sentences, target_sentences = prepare_dataset(valid_dataset, 30, prompt)\n",
        "\n",
        "#print(\"Actual Translation: \", target_sentences)\n",
        "\n",
        "# Baseline model predictions\n",
        "output_predictions_google = translate_google(source_sentences)\n",
        "#print(\"Google translations: \",output_predictions_google)\n",
        "output_predictions_llama = translate_llama(source_sentences)\n",
        "#print(\"Llama translations: \",output_predictions_llama)\n",
        "\n",
        "print(\"--------- Google Translate Results --------\")\n",
        "print(\"Meteor Score: \", evaluation_score(metric=\"meteor\", predictions=output_predictions_google, references=target_sentences))\n",
        "print(\"BLEU Score: \", evaluation_score(metric=\"bleu\", predictions=output_predictions_google, references=target_sentences))\n",
        "\n",
        "print(\"--------- Llama3 Results --------\")\n",
        "print(\"Meteor Score: \", evaluation_score(metric=\"meteor\", predictions=output_predictions_llama, references=target_sentences))\n",
        "print(\"BLEU Score: \", evaluation_score(metric=\"bleu\", predictions=output_predictions_llama, references=target_sentences))"
      ],
      "metadata": {
        "id": "R6tuH1fOBX1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  BLOOMZ Model Translations"
      ],
      "metadata": {
        "id": "nn7PTZMTJBjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Translate to Hindi: {}. Translation:\"\n",
        "prompt_inputs, source_sentences, target_sentences = prepare_dataset(valid_dataset, 30, prompt)\n",
        "\n",
        "def model_prediction(inputs, pipe, batch_size, max_length=10000, truncation=\"do_not_truncate\"):\n",
        "  output_pred=[]\n",
        "\n",
        "  for out in pipe(inputs, batch_size=batch_size, max_length=max_length, truncation=truncation):\n",
        "      output_pred.append(out[0]['generated_text'].split(\"Translation:\")[-1].strip())\n",
        "\n",
        "  return output_pred\n",
        "\n",
        "#print(\"Actual Translation: \", target_sentences)\n",
        "pipe = pipeline(\"text-generation\", model= \"bigscience/bloomz-560m\",device=device)\n",
        "output_predictions = model_prediction(prompt_inputs, pipe, 1)\n",
        "#print(\"Model Translations: \", output_predictions)\n",
        "\n",
        "print(\"--------- Bloomz Results --------\")\n",
        "print(\"Meteor Score: \", evaluation_score(metric=\"meteor\", predictions=output_predictions, references=target_sentences))\n",
        "print(\"BLEU Score: \", evaluation_score(metric=\"bleu\", predictions=output_predictions, references=target_sentences))\n",
        "\n"
      ],
      "metadata": {
        "id": "Vi-omU9uuZzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MBart-50 Model Translations"
      ],
      "metadata": {
        "id": "NQQVfmbUSR5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "prompt = \"Translate to Hindi: {}. Translation:\"\n",
        "prompt_inputs, source_sentences, target_sentences = prepare_dataset(valid_dataset, 30, prompt)\n",
        "\n",
        "mbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\").to(device)\n",
        "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "def mbart_translations(model, tokenizer, source_sentences, single_sentence=False):\n",
        "  model_predictions = []\n",
        "  if single_sentence == False:\n",
        "    for input in source_sentences:\n",
        "\n",
        "      tokenizer.src_lang = \"en_XX\"\n",
        "      encoded_hi = tokenizer(input, return_tensors=\"pt\").to(device)\n",
        "      generated_tokens = model.generate(\n",
        "          **encoded_hi,\n",
        "          forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n",
        "      )\n",
        "      res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "      model_predictions.append(res)\n",
        "  else:\n",
        "    tokenizer.src_lang = \"en_XX\"\n",
        "    encoded_hi = tokenizer(source_sentences, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = model.generate(\n",
        "        **encoded_hi,\n",
        "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n",
        "    )\n",
        "    res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "    model_predictions.append(res)\n",
        "\n",
        "  return model_predictions\n",
        "\n",
        "model_predictions = mbart_translations(mbart_model, mbart_tokenizer, source_sentences)\n",
        "#print(\"Actual Translation: \", target_sentences)\n",
        "\n",
        "#print(\"Model Translations: \", model_predictions)\n",
        "\n",
        "print(\"--------- MBart Results --------\")\n",
        "print(\"Meteor Score: \", evaluation_score(metric=\"meteor\", predictions=model_predictions, references=target_sentences))\n",
        "print(\"BLEU Score: \", evaluation_score(metric=\"bleu\", predictions=model_predictions, references=target_sentences))\n"
      ],
      "metadata": {
        "id": "3LCQH70GFnx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLLB Model Translations"
      ],
      "metadata": {
        "id": "BO_NsN0NSdwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "prompt = \"Translate to Hindi: {}. Translation:\"\n",
        "prompt_inputs, source_sentences, target_sentences = prepare_dataset(valid_dataset, 30, prompt)\n",
        "\n",
        "# Load model directly\n",
        "nllb_tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
        "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").to(device)\n",
        "\n",
        "def nllb_translations(model, tokenizer, source_sentences, single_sentence=False):\n",
        "  model_predictions = []\n",
        "  if single_sentence == False:\n",
        "    for input in source_sentences:\n",
        "      inputs = tokenizer(input, return_tensors=\"pt\").to(device)\n",
        "\n",
        "      translated_tokens = model.generate(\n",
        "          **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"hin_Deva\"), max_length=50\n",
        "      )\n",
        "\n",
        "      res = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "      model_predictions.append(res)\n",
        "  else:\n",
        "    inputs = tokenizer(source_sentences, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    translated_tokens = model.generate(\n",
        "        **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"hin_Deva\"), max_length=50\n",
        "    )\n",
        "\n",
        "    res = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "    model_predictions.append(res)\n",
        "\n",
        "  return model_predictions\n",
        "\n",
        "model_predictions = nllb_translations(nllb_model, nllb_tokenizer, source_sentences)\n",
        "\n",
        "#print(\"Actual Translation: \", target_sentences)\n",
        "\n",
        "#print(\"Model Translations: \", model_predictions)\n",
        "\n",
        "print(\"---------NLLB Results --------\")\n",
        "print(\"Meteor Score: \", evaluation_score(metric=\"meteor\", predictions=model_predictions, references=target_sentences))\n",
        "print(\"BLEU Score: \", evaluation_score(metric=\"bleu\", predictions=model_predictions, references=target_sentences))\n",
        "\n"
      ],
      "metadata": {
        "id": "pjppRx86JZN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### M2M100 Model Translations"
      ],
      "metadata": {
        "id": "tZUuAik_n_2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "\n",
        "m2m100_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\").to(device)\n",
        "m2m100_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
        "\n",
        "def m2m100_translations(model, tokenizer, source_sentences, single_sentence=False):\n",
        "  model_predictions = []\n",
        "  if single_sentence == False:\n",
        "    for input in source_sentences:\n",
        "      tokenizer.src_lang = \"en\"\n",
        "      encoded_hi = tokenizer(input, return_tensors=\"pt\").to(device)\n",
        "      generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"hi\"))\n",
        "      res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "      model_predictions.append(res)\n",
        "  else:\n",
        "    tokenizer.src_lang = \"en\"\n",
        "    encoded_hi = tokenizer(source_sentences, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"hi\"))\n",
        "    res = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "    model_predictions.append(res)\n",
        "\n",
        "  return model_predictions\n",
        "\n",
        "model_predictions = nllb_translations(m2m100_model, m2m100_tokenizer, source_sentences)\n",
        "\n",
        "#print(\"Actual Translation: \", target_sentences)\n",
        "\n",
        "#print(\"Model Translations: \", model_predictions)\n",
        "\n",
        "print(\"---------M2M100 Results --------\")\n",
        "print(\"Meteor Score: \", evaluation_score(metric=\"meteor\", predictions=model_predictions, references=target_sentences))\n",
        "print(\"BLEU Score: \", evaluation_score(metric=\"bleu\", predictions=model_predictions, references=target_sentences))\n",
        "\n"
      ],
      "metadata": {
        "id": "dzu1rSMBl4qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translation Comparision using Test Sentence by User"
      ],
      "metadata": {
        "id": "5zzvrwd8wMAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "user_input = input(\"Enter text to translate to Hindi: \")\n",
        "\n",
        "#bloomz_pred = model_prediction(prompt.format(input), pipe, 1)\n",
        "nllb_pred = nllb_translations(nllb_model, nllb_tokenizer, user_input, single_sentence=True)\n",
        "mbart_pred = mbart_translations(mbart_model, mbart_tokenizer, user_input, single_sentence=True)\n",
        "m2m100_pred = m2m100_translations(m2m100_model, m2m100_tokenizer, user_input, single_sentence=True)\n",
        "\n",
        "#print(bloomz_pred)\n",
        "print(\"NLLB \",nllb_pred)\n",
        "print(\"MBart \",mbart_pred)\n",
        "print(\"M2M100 \",m2m100_pred)\n",
        "\n",
        "\n",
        "google_pred = translate_google(user_input, single_sentence=True)\n",
        "llama_pred = translate_llama(user_input, single_sentence=True)\n",
        "\n",
        "print(\"Google Translate \",google_pred)\n",
        "print(\"Llama \",llama_pred)\n"
      ],
      "metadata": {
        "id": "ad-I0TBsRpQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3MznN9cu_ID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}